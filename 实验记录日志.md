# 实验记录 Log（Mamba2 / H100）

## 基本信息
- 记录日期：2026-02-17
- 工作目录：`/nfs/hpc/share/zhanhaoc/hpe/tempopeak`
- 计算资源：`NVIDIA H100 80GB HBM3`

## 过程记录

| 时间 | 目的 | 行为 | 原因 | 结果 |
|---|---|---|---|---|
| 2026-02-17（终端序号 973） | 激活实验环境 | `source /nfs/stak/users/zhanhaoc/hpc-share/conda/bin/activate`；`conda activate sam_3d_body` | 保证依赖与 CUDA 配置一致 | 环境激活成功，提示符为 `(sam_3d_body)` |
| 2026-02-17（终端序号 974） | 验证 mamba-ssm 安装 | `python -c "import mamba_ssm; print(mamba_ssm.__version__)"` | 确认 Mamba2 可用 | 版本 `2.2.6.post3` |
| 2026-02-17（终端序号 974） | 验证 PyTorch 与 GPU | `python -c "import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"` | 排除 CUDA/设备不可用问题 | `2.9.1+cu126`，`True`，`NVIDIA H100 80GB HBM3` |
| 2026-02-17（终端序号 976） | 功能冒烟测试 | 运行 `python smoke_mamba.py`（`B=2,T=64,D=128`） | 验证前向是否正确、形状是否保持 | 输入输出均为 `torch.Size([2, 64, 128])`，前向成功 |
| 2026-02-17（终端序号 977） | 前向性能基准 | 运行 50 次前向（`B=8,T=64,D=128`）并统计平均时延 | 评估时序模块是否为瓶颈 | `avg time: 0.003558688163757324`（约 `3.56 ms/forward`） |
| 2026-02-19 | 实现 BiMamba 模型 | 编写 `models/model.py` (ResNet18+BiMamba) 和 `test_model.py` | 搭建 ResNet+BiMamba 基础结构 | 代码完成，本地无 CUDA 无法冒烟，待 HPC 验证 |
| 2026-02-19 | HPC 训练冒烟测试 | 运行 `train_smoke.py` (CrossEntropy + AdamW) | 验证梯度流和 loss 收敛性 | Loss 4.15 -> 0.00，Acc -> 1.0。验证通过。 |
| 2026-02-19 | 优化模型输出 | 修改 `models/model.py` 返回 logits (移除 softmax) | 适配 PyTorch `CrossEntropyLoss` | 现在模型输出无界 logits，需在外部做 softmax |
| 2026-02-20 | 修复训练脚本 | 将 `train.py` 中的合成数据生成移出循环外 | 循环内随机数据导致无法收敛(loss停在log(64)) | 修复后期待可完美 overfit 一个 batch，验证模型拟合能力 |

## 结论
- `mamba2` 安装与 CUDA Kernel 正常。
- 当前配置下时序模块开销较小，不是主要计算瓶颈。
- 下一步：先实现 `Identity baseline`，再集成 `Bi-Mamba temporal head`。
